!/bin/bash
#SBATCH --job-name=ir2
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --ntasks-per-node=1
#SBATCH --time=12:00:00
#SBATCH --mem=30000M
#SBATCH --partition=gpu_shared_course
#SBATCH --gres=gpu:1


module purge
module load pre2019
module load eb

#module load python/2.7.9
#module load python/3.5.0
module load cuDNN/7.1-CUDA-8.0.44-GCCcore-5.4.0
module load CUDA/8.0.44-GCCcore-5.4.0
export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.1-CUDA-8.0.44-GCCcore-5.4.0/lib64:$LD_LIBRARY_PATH

module load Anaconda2
source activate time_lstm


MODEL="TLSTM3"
BATCH=5
TEST_BATCH=5
VOCAB=5000
MLEN=200
DATA='music'
FIXED_EPOCHS=10
NUM_EPOCHS=50
NHIDDEN=128
PRETRAINED=""
LAST_EPOCH=9
SAMPLE_TIME=3
LEARNING_RATE=0.01
FLAGS="floatX=float32,device=cuda0, gpuarray.preallocate=1"
THEANO_FLAGS="${FLAGS}" python2 main.py --model TLSTM3   --data ${DATA} --batch_size ${BATCH} --vocab_size ${VOCAB} --max_len ${MLEN} --fixed_epochs ${FIXED$
